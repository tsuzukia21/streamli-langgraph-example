import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_community.vectorstores import FAISS
from langgraph.graph import StateGraph
from typing import List, Literal, TypedDict
from langgraph.graph import END, StateGraph, START
import asyncio
import os
from langgraph.constants import Send

class RouteQuery(BaseModel):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ã‹åˆ¤æ–­ã—ã¾ã™ã€‚"""

    choice: Literal["vectorstore", "no_tool_use"] = Field(
        ...,
        description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ã‹ã—ãªã„ã‹",
    )
    reason: str = Field(
        ...,
        description="åˆ¤æ–­ç†ç”±ã®èª¬æ˜",
    )
    answer: str = Field(
        ...,
        description="è¿½åŠ ã®å›ç­”æƒ…å ±ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰",
    )
    query: str = Field(
        ...,
        description="ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹å ´åˆã¯ã€è³ªå•ã‚’ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«é©ã—ãŸã‚¯ã‚¨ãƒªã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚",
    )

class GradeDocuments(BaseModel):
    """å–å¾—ã•ã‚ŒãŸæ–‡æ›¸ã®é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯ã®ãŸã‚ã®ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã€‚"""
    binary_score: str = Field(
        description="æ–‡æ›¸ãŒè³ªå•ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã€ã€Œyesã€ã¾ãŸã¯ã€Œnoã€"
    )

class GradeHallucinations(BaseModel):
    """ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã«ãŠã‘ã‚‹å¹»è¦šã®æœ‰ç„¡ã‚’ç¤ºã™ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã€‚"""
    binary_score: str = Field(
        description="å›ç­”ãŒäº‹å®Ÿã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ã©ã†ã‹ã€ã€Œyesã€ã¾ãŸã¯ã€Œnoã€"
    )

class GradeAnswer(BaseModel):
    """å›ç­”ãŒè³ªå•ã«å¯¾å‡¦ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã™ã‚‹ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã€‚"""
    binary_score: str = Field(
        description="å›ç­”ãŒè³ªå•ã«å¯¾å‡¦ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã€ã€Œyesã€ã¾ãŸã¯ã€Œnoã€"
    )

class GraphState(TypedDict):
    """
    ã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹ã‚’è¡¨ã—ã¾ã™ã€‚
    å±æ€§:
        question: è³ªå•
        query: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        generation: LLMç”Ÿæˆ
        documents: æ–‡æ›¸ã®ãƒªã‚¹ãƒˆ
    """
    question: str
    query: str
    generation: str
    documents: List[str]

def update_status_and_messages(message: str, state: str = "running", expanded: bool = True, additional_info: str = ""):
    """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸€æ‹¬æ›´æ–°ã™ã‚‹é–¢æ•°
    Args:
        message (str): è¡¨ç¤ºã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        state (str, optional): ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®çŠ¶æ…‹. Defaults to "running".
        expanded (bool, optional): ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å±•é–‹ã™ã‚‹ã‹ã©ã†ã‹. Defaults to True.
        additional_info (str, optional): ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã«è¡¨ç¤ºã™ã‚‹è¿½åŠ æƒ…å ±. Defaults to "".
    """
    st.session_state.status.update(label=f"{message}", state=state, expanded=expanded)
    if additional_info:
        st.session_state.placeholder.markdown(additional_info)
    st.session_state.status_messages += message + "\n\n"
    if additional_info:
        st.session_state.status_messages += additional_info + "\n\n"

async def route_question(state):
    """
    è³ªå•ã‚’åˆ†æã—ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹åˆ¤å®šã™ã‚‹é–¢æ•°
    vectorstore: ç¤¾å†…è¦å®šã«é–¢ã™ã‚‹è³ªå•
    no_tool_use: ãã®ä»–ã®è³ªå•
    """
    update_status_and_messages(
        "**---ROUTE QUESTION---**",
        expanded=False,
    )
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    structured_llm_router = llm.with_structured_output(RouteQuery)
    system = """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã‚’åˆ†æã—ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚å›ç­”ã¯ä»¥ä¸‹ã®å½¢å¼ã§æä¾›ã—ã¦ãã ã•ã„ï¼š

1. choice: "vectorstore" ã¾ãŸã¯ "no_tool_use" ã‚’é¸æŠ
2. reason: é¸æŠã—ãŸç†ç”±ã‚’ç°¡æ½”ã«èª¬æ˜
3. answer: è¿½åŠ æƒ…å ±(å¿…è¦ãªå ´åˆã®ã¿)
4. query: ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹å ´åˆã¯ã€è³ªå•ã‚’ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«é©ã—ãŸè³ªå•ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚

ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢(vectorstore)ã«ã¯ç¤¾å†…è¦å®šãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã®å ´åˆã¯ "vectorstore" ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š
- ç‰¹å®šã®æ¡æ–‡ã®å†…å®¹ã‚’å¼•ç”¨ãƒ»å‚ç…§ã™ã‚‹è³ªå•ã€‚ãŸã ã—ã€è¤‡æ•°ã®æ¡æ–‡ã®æ¯”è¼ƒã‚„è§£é‡ˆã€å¤–éƒ¨æƒ…å ±ã¨ã®ç…§åˆãŒå¿…è¦ãªå ´åˆã¯ã€"no_tool_use" ã‚’é¸æŠã™ã‚‹ã€‚
- ç‰¹å®šã®ç”¨èªã®å®šç¾©ã«é–¢ã™ã‚‹è³ªå•
- ç¤¾å†…è¦å®šã‚„ã«ç›´æ¥é–¢é€£ã™ã‚‹è³ªå•

ä»¥ä¸‹ã®å ´åˆã¯ "no_tool_use" ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š
- åºƒç¯„å›²ã«æ¸¡ã‚‹èª¿æŸ»ã‚„åˆ†æãŒå¿…è¦ãªè³ªå•
- è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’å‚ç…§ãƒ»æ¯”è¼ƒã™ã‚‹å¿…è¦ãŒã‚ã‚‹è³ªå•
- ä¸»è¦³çš„ãªåˆ¤æ–­ã‚„å‰µé€ æ€§ã‚’å¿…è¦ã¨ã™ã‚‹è³ªå•
- å°‚é–€çš„ãªçŸ¥è­˜ã‚„çµŒé¨“ãŒå¿…è¦ãªè³ªå•
- æ³•å¾‹ã‚„å€«ç†ã«é–¢ã‚ã‚‹è¤‡é›‘ãªè³ªå•
- ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®å†…å®¹ã¨æ˜ã‚‰ã‹ã«ç„¡é–¢ä¿‚ãªè³ªå•

"no_tool_use"ã‚’é¸æŠã—ãŸå ´åˆanswerã¯ä»¥ä¸‹ã®å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚å°‚é–€ç”¨èªã¯ä½¿ã‚ãšã«å›ç­”ã™ã‚‹ã“ã¨ã€‚:
- é€šå¸¸ã®ä¼šè©±: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ—¥å¸¸çš„ãªä¼šè©±ã‚„é›‘è«‡ã‚’ã—ã¦ã„ã‚‹å ´åˆã¯ã€ãã‚Œã«åˆã‚ã›ã¦ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã«ä¼šè©±ã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚
- æ›–æ˜§ãªè³ªå•ã¸ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ãŒæ›–æ˜§ã§ã€ä½•ã‚’æ±‚ã‚ã¦ã„ã‚‹ã®ã‹æ˜ç¢ºã§ãªã„å ´åˆã¯ã€ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã‚’ã™ã‚‹ã‚ˆã†ã«åˆ†ã‹ã‚Šã‚„ã™ãã‚¢ãƒ‰ãƒã‚¤ã‚¹ã—ã¦ãã ã•ã„ã€‚
- å°‚é–€çš„ãªè³ªå•ã¸ã®å¯¾å¿œ: å°‚é–€çŸ¥è­˜ãŒå¿…è¦ãªè³ªå•ã«å¯¾ã—ã¦ã¯ã€å°‚é–€å®¶ã‚„æ‹…å½“è€…ã¸ã®ç›¸è«‡ã‚’ä¿ƒã™ãªã©ã€é©åˆ‡ãªå¯¾å¿œã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
- å¯¾å¿œã§ããªã„è³ªå•ã¸ã®å¯¾å¿œ: å€«ç†çš„ãªå•é¡Œã‚„ã€ã‚ãªãŸã®èƒ½åŠ›ã‚’è¶…ãˆã‚‹è³ªå•ã«å¯¾ã—ã¦ã¯ã€å¯¾å¿œã§ããªã„æ—¨ã‚’åˆ†ã‹ã‚Šã‚„ã™ãä¼ãˆã¦ä¸‹ã•ã„ã€‚

ä»¥ä¸‹ã¯ä¾‹ã§ã™ï¼š
question: åˆå¹´åº¦ã«ä¸ãˆã‚‰ã‚Œã‚‹æœ‰çµ¦ä¼‘æš‡ã¯ä½•æ—¥ã‹æ•™ãˆã¦ãã ã•ã„ã€‚
choice: "vectorstore"
reason: "ã“ã®è³ªå•ã§ã¯ã€ç¤¾å†…è¦å®šã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å…·ä½“çš„ãªæ‰‹é †ã«ã¤ã„ã¦å°‹ã­ã¦ã„ã¾ã™ã€‚"
answer: ""
query: "æœ‰çµ¦ä¼‘æš‡ æ—¥æ•°"

question: ä»¥å‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç¤¾å†…è¦ç¨‹ã‹ã‚‰ã®å¤‰æ›´ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ
choice: "no_tool_use"
reason: "ã“ã®è³ªå•ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ç¯„å›²å¤–ã¨ãªã‚‹åºƒç¯„ãªèª¿æŸ»ãŒå¿…è¦ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
answer: "ã“ã®ã‚¢ãƒ—ãƒªã®ç¯„å›²å¤–ã¨ãªã‚‹åºƒç¯„ãªèª¿æŸ»ãŒå¿…è¦ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ç›´æ¥æ‹…å½“è€…ã¸ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
query: ""

question: æ–°è»Šã®ãŠã™ã™ã‚ã¯ä½•ã§ã™ã‹ï¼Ÿ
choice: "no_tool_use"
reason: "ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®å†…å®¹ã¨æ˜ã‚‰ã‹ã«ç„¡é–¢ä¿‚ãªè³ªå•ã§ã™ã€‚"
answer: "ã“ã®ã‚¢ãƒ—ãƒªã®ç¯„å›²å¤–ã¨ãªã‚‹è³ªå•ã¨æ¨æ¸¬ã•ã‚Œã¾ã™ã€‚ç›´æ¥æ‹…å½“è€…ã¸ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
query: ""
"""

    route_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "{question}"),
        ]
    )

    question_router = route_prompt | structured_llm_router

    question = state["question"]
    source = question_router.invoke({"question": "question:" + question})
    if source.choice  == "no_tool_use":
        update_status_and_messages(
            "NOT ROUTE QUESTION TO RAG",
        )
        return Send("no_tool_use",{"question": question, "generation": source.answer})
    elif source.choice  == "vectorstore":
        update_status_and_messages(
            "ROUTE QUESTION TO RAG",
            additional_info=f"Query: {source.query}"
        )
        return Send("retrieve",{"question": question,"query": source.query})

async def no_tool_use(state):
    return state

async def retrieve(state):
    """
    FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ã™ã‚‹é–¢æ•°
    """
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    update_status_and_messages(
        "**---RETRIEVE---**",
        additional_info=f"RETRIEVINGâ€¦\n\nKEY WORD:{state['query']}"
    )
    file="employment_rules"
    db=FAISS.load_local(file, embeddings,allow_dangerous_deserialization=True)

    retriever = db.as_retriever(search_kwargs={'k': 6})
    query = state["query"]
    documents = retriever.invoke(query)
    update_status_and_messages(
        "**RETRIEVE SUCCESS!!**",
        state="complete",
    )
    state["documents"] = documents
    return state

async def grade_documents(state):
    """
    æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸ãŒè³ªå•ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹é–¢æ•°
    2å›ç›®ã®è©¦è¡Œã¾ã§ã¯æ–‡æ›¸ã®é–¢é€£æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    """
    st.session_state.number_trial += 1
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    structured_llm_grader = llm.with_structured_output(GradeDocuments)

    system = """ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦å–å¾—ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é–¢é€£æ€§ã‚’è©•ä¾¡ã™ã‚‹æ¡ç‚¹è€…ã§ã™ã€‚
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„æ„å‘³ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ãã‚Œã‚’é–¢é€£æ€§ãŒã‚ã‚‹ã¨è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
ç›®çš„ã¯æ˜ã‚‰ã‹ã«èª¤ã£ãŸå–å¾—ã‚’æ’é™¤ã™ã‚‹ã“ã¨ã§ã™ã€‚å³å¯†ãªãƒ†ã‚¹ãƒˆã§ã‚ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè³ªå•ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¤ºã™ãŸã‚ã«ã€ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã€Œyesã€ã¾ãŸã¯ã€Œnoã€ã‚’ä¸ãˆã¦ãã ã•ã„ã€‚"""
    grade_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "Retrieved document: \n\n {document} \n\n User question: {question}"),
        ]
    )

    retrieval_grader = grade_prompt | structured_llm_grader
    update_status_and_messages(
        "**---CHECK DOCUMENT RELEVANCE TO QUESTION---**",
        expanded=False
    )       
    question = state["question"]
    documents = state["documents"]
    filtered_docs = []
    i = 0
    for d in documents:
        if st.session_state.number_trial <= 2:
            file_name = d.metadata["source"]
            file_name = os.path.basename(file_name.replace("\\","/"))
            page = d.metadata["page"]
            content = d.page_content
            i += 1
            score = retrieval_grader.invoke(
                {"question": question, "document": d.page_content}
            )
            grade = score.binary_score
            if grade == "yes":
                update_status_and_messages(
                    "**GRADE: DOCUMENT RELEVANT**",
                    additional_info=f"DOC {i}/{len(documents)} {file_name} page.{page} : **RELEVANT**\n\n{content}",
                    state="complete",
                )
                filtered_docs.append(d)
            else:
                update_status_and_messages(
                    "**GRADE: DOCUMENT NOT RELEVANT**",
                    state="error",
                    additional_info=f"DOC {i}/{len(documents)} {file_name} page.{page} : **NOT RELEVANT**\n\n{content}"
                )
        else:
            filtered_docs.append(d)

    if not st.session_state.number_trial <= 2:
        update_status_and_messages(
            "**NO NEED TO CHECK**",
            state="complete",
            expanded=False
        )
        update_status_and_messages(
            "**QUERY TRANSFORMATION HAS BEEN COMPLETED**",
            state="complete",
            expanded=False
        )
    state["documents"] = filtered_docs
    return state

async def generate(state):
    """
    æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸ã‚’åŸºã«å›ç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    """
    update_status_and_messages(
        "**---GENERATE---**",
        expanded=False
    )
    prompt = ChatPromptTemplate.from_messages(
            [
                ("system", """ã‚ãªãŸã¯ã¨ã‚ã‚‹ä¼šç¤¾ã®ç¤¾å†…è¦å®šã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®å–å¾—ã•ã‚ŒãŸæ–‡æ›¸ã‚’ä½¿ç”¨ã—ã¦ç¤¾å“¡ã‹ã‚‰ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
æ–‡æ›¸ã¯è³ªå•ã«å¯¾ã—ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã—ã¦å–å¾—ã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚å¿…ãšã—ã‚‚æœ€é©ãªæ–‡æ›¸ãŒã‚ã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“ãŒã€ãã®çµæœã‚’åŸºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
ç­”ãˆãŒã‚ã‹ã‚‰ãªã„å ´åˆã‚„æ–‡æ›¸ãŒä¸ååˆ†ãªå ´åˆã¯ã€å˜˜ã®å›ç­”ã‚’ä½œã£ãŸã‚Šã›ãšã«æ‹…å½“éƒ¨ç½²ã¸ç›´æ¥å•ã„åˆã‚ã›ã‚‹ã‚ˆã†ä¼ãˆã¦ãã ã•ã„ã€‚
å›ç­”ã«æ–‡æ›¸ã‚’å‚ç…§ã—ãŸå ´åˆã«ã¯ã€å›ç­”ã®æœ€å¾Œã«ã¯å‚ç…§ã—ãŸæ–‡æ›¸åã€ãƒšãƒ¼ã‚¸ã€æ–‡æ›¸ã®æ”¹å®šæ—¥ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚"""),
                ("human", """Question: {question} 
Context: {context}"""),
            ]
        )
        
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

    rag_chain = prompt | llm | StrOutputParser()
    question = state["question"]
    documents = state["documents"]
    generation = rag_chain.invoke({"context": documents, "question": question})
    state["generation"] = generation
    return state

async def transform_query(state):
    """
    æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦å†æ¤œç´¢ã‚’è¡Œã†é–¢æ•°
    ã‚ˆã‚Šè‰¯ã„æ¤œç´¢çµæœã‚’å¾—ã‚‹ãŸã‚ã«ã‚¯ã‚¨ãƒªã‚’æ›¸ãæ›ãˆã‚‹
    """
    update_status_and_messages(
        "**---TRANSFORM QUERY---**",
        expanded=True
    )
    st.session_state.placeholder.empty()
    llm = ChatOpenAI(model="gpt-4o", temperature=0)

    system = """ã‚ãªãŸã¯ã€æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ¤œç´¢ã«æœ€é©åŒ–ã•ã‚ŒãŸã‚ˆã‚Šè‰¯ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¤‰æ›ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ã“ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸç¤¾å†…è¦å®šã«å¯¾ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
ä¸€å›ç›®ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ã€è‰¯ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—å‡ºæ¥ãªã‹ã£ãŸã®ã§å†æŒ‘æˆ¦ã§ã™ã€‚
è³ªå•ã‚’è¦‹ã¦ã€è³ªå•è€…ã®æ„å›³/æ„å‘³ã«ã¤ã„ã¦æ¨è«–ã—ã¦ã‚ˆã‚Šè‰¯ã„ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ç‚ºã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’1å€‹ã®ã¿æ–‡å­—åˆ—ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
    re_write_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            (
                "human",
                "æœ€åˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: \n{query}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•:\n{question}\n\næ”¹å–„ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚",
            ),
        ]
    )

    question_rewriter = re_write_prompt | llm | StrOutputParser()
    question = state["question"]
    query = state["query"]
    better_question = question_rewriter.invoke({"query": query ,"question": question})
    update_status_and_messages(
        f"**Better question: {better_question}**",
        state="complete",
    )
    state["query"] = better_question
    return state

async def decide_to_generate(state):
    """
    æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¯ã‚¨ãƒªã‚’å¤‰æ›ã—ã€
    æ–‡æ›¸ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯å›ç­”ç”Ÿæˆã«é€²ã‚€åˆ¤æ–­ã‚’è¡Œã†é–¢æ•°
    """
    filtered_documents = state["documents"]
    if not filtered_documents:
        update_status_and_messages(
            "**DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY**",
            state="error",
            expanded=False
        )
        return "transform_query"                                     
    else:
        update_status_and_messages(
            "**DECISION: GENERATE**",
            state="complete",
            expanded=False
        )
        return "generate"

async def grade_generation_v_documents_and_question(state):
    """
    ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã®å“è³ªã‚’è©•ä¾¡ã™ã‚‹é–¢æ•°
    - æ–‡æ›¸ã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ï¼ˆå¹»è¦šãŒãªã„ã‹ï¼‰
    - è³ªå•ã«é©åˆ‡ã«ç­”ãˆã¦ã„ã‚‹ã‹
    ã‚’ãƒã‚§ãƒƒã‚¯
    """
    st.session_state.number_trial += 1  
    update_status_and_messages(
        "**---CHECK HALLUCINATIONS---**",
        expanded=False
    )
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    structured_llm_grader = llm.with_structured_output(GradeHallucinations)

    system = """ã‚ãªãŸã¯ã€LLMã®ç”ŸæˆãŒå–å¾—ã•ã‚ŒãŸäº‹å®Ÿã®ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹ã‹/ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹æ¡ç‚¹è€…ã§ã™ã€‚
ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã€Œyesã€ã¾ãŸã¯ã€Œnoã€ã‚’ä¸ãˆã¦ãã ã•ã„ã€‚ã€Œyesã€ã¯ã€å›ç­”ãŒäº‹å®Ÿã®ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹/ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚"""
    hallucination_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "Set of facts: \n\n {documents} \n\n LLM generation: {generation}"),
        ]
    )
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    structured_llm_grader = llm.with_structured_output(GradeAnswer)

    system = """ã‚ãªãŸã¯ã€å›ç­”ãŒè³ªå•ã«å¯¾å‡¦ã—ã¦ã„ã‚‹ã‹/è§£æ±ºã—ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹æ¡ç‚¹è€…ã§ã™ã€‚
ãƒã‚¤ãƒŠãƒªã‚¹ã‚³ã‚¢ã€Œyesã€ã¾ãŸã¯ã€Œnoã€ã‚’ä¸ãˆã¦ãã ã•ã„ã€‚ã€Œyesã€ã¯ã€å›ç­”ãŒè³ªå•ã‚’è§£æ±ºã—ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚"""
    answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system),
            ("human", "User question: \n\n {question} \n\n LLM generation: {generation}"),
        ]
    )

    answer_grader = answer_prompt | structured_llm_grader
    hallucination_grader = hallucination_prompt | structured_llm_grader
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]
    score = hallucination_grader.invoke(
        {"documents": documents, "generation": generation}
    )
    grade = score.binary_score
    if st.session_state.number_trial <= 3:
        if grade == "yes":
            update_status_and_messages(
                "**DECISION: ANSWER IS BASED ON A SET OF FACTS**",
                state="complete",
            )
            update_status_and_messages(
                "**---GRADE GENERATION vs QUESTION---**",
            )
            score = answer_grader.invoke({"question": question, "generation": generation})
            grade = score.binary_score
            if grade == "yes":
                update_status_and_messages(
                    "**DECISION: GENERATION ADDRESSES QUESTION**",
                    state="complete",
                )
                with st.session_state.placeholder:
                    update_status_and_messages(
                        "**USEFUL!!**",
                        additional_info=f"question : {question}\n\ngeneration : {generation}",
                        state="complete",
                    )
                return "useful"
            else:
                st.session_state.number_trial -= 1
                update_status_and_messages(
                    "**DECISION: GENERATION DOES NOT ADDRESS QUESTION**",
                    additional_info=f"question:{question}\n\ngeneration:{generation}",
                    state="error",
                )
                with st.session_state.placeholder:
                    update_status_and_messages(
                        "**NOT USEFUL**",
                        additional_info=f"question:{question}\n\ngeneration:{generation}",
                        state="error",
                    )
                return "not useful"
        else:
            message = "**DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY**"
            st.session_state.status.update(label=f"{message}", state="error", expanded=True)
            with st.session_state.placeholder:
                update_status_and_messages(
                    "**NOT GROUNDED**",
                    additional_info=f"question:{question}\n\ngeneration:{generation}",
                    state="error",
                )
            return "not supported"
    else:
        update_status_and_messages(
            "**NO NEED TO CHECK**",
            expanded=False,
            state="complete",
        )
        update_status_and_messages(
            "**TRIAL LIMIT EXCEEDED**",
            expanded=False,
            state="complete",
        )
        return "useful"

async def run_workflow(inputs):
    """
    å…¨ä½“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°
    çŠ¶æ…‹ç®¡ç†ã¨UIã®æ›´æ–°ã‚’è¡Œã†
    """
    st.session_state.number_trial = 0
    with st.status(label="**GO!!**", expanded=True,state="running") as st.session_state.status:
        st.session_state.placeholder = st.empty()
        value = await st.session_state.workflow.ainvoke(inputs)

    st.session_state.placeholder.empty()
    st.session_state.answer = st.empty()
    update_status_and_messages(
        "**FINISH!!**",
        state="complete",
        expanded=False,
    )
    st.session_state.answer.markdown(value["generation"])
    with st.popover("ãƒ­ã‚°"):
        st.markdown(st.session_state.status_messages)

# ãƒ¡ã‚¤ãƒ³ã®Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³éƒ¨åˆ†
if 'status_messages' not in st.session_state:
    st.session_state.status_messages = ""

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®åˆæœŸåŒ–ï¼ˆåˆå›ã®ã¿å®Ÿè¡Œï¼‰
if not hasattr(st.session_state, "workflow"):
    # StateGraphã®è¨­å®š
    workflow = StateGraph(GraphState)
    # ãƒãƒ¼ãƒ‰ã®è¿½åŠ 
    workflow.add_node("no_tool_use", no_tool_use)
    workflow.add_node("retrieve", retrieve)
    workflow.add_node("grade_documents", grade_documents)
    workflow.add_node("generate", generate)
    workflow.add_node("transform_query", transform_query)

    # ã‚¨ãƒƒã‚¸ã®è¿½åŠ ï¼ˆãƒ•ãƒ­ãƒ¼ã®åˆ¶å¾¡ï¼‰
    workflow.add_conditional_edges(
        START,
        route_question,
        {
            "retrieve": "retrieve",
            "no_tool_use": "no_tool_use",
        },
    )
    workflow.add_edge("retrieve", "grade_documents")
    workflow.add_conditional_edges(
        "grade_documents",
        decide_to_generate,
        {
            "transform_query": "transform_query",
            "generate": "generate",
        },
    )
    workflow.add_edge("transform_query", "retrieve")
    workflow.add_edge("no_tool_use", END)
    workflow.add_conditional_edges(
        "generate",
        grade_generation_v_documents_and_question,
        {
            "not supported": "transform_query",
            "useful": END,
            "not useful": "transform_query",
        },
    )
    app = workflow.compile()
    app = app.with_config(recursion_limit=20,run_name="Agent",tags=["Agent","FB"])
    app.name = "Agent"
    st.session_state.workflow = app

st.title("è‡ªå·±ä¿®æ­£RAG")
st.write("ç¤¾å†…è¦å®šã«é–¢ã™ã‚‹**ä¸€å•ä¸€ç­”å½¢å¼**ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚")

if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    st.session_state.status_messages = ""
    with st.chat_message("user", avatar="ğŸ˜Š"):
        st.markdown(prompt)

    inputs = {"question": prompt}
    asyncio.run(run_workflow(inputs))